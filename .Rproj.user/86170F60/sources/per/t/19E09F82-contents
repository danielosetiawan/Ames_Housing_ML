---
title: "Introduction to Time Series"
author: "NYC Data Science Academy"
date: "6/24/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


```{r packages, echo=F, message=F}
library(datasets)
library(forecast)
library(astsa)
# install.packages("devtools")
# devtools::install_github("FinYang/tsdl")
library(tsdl)

# install.packages('tsdl')
set.seed(444)
```
## Introduction to Time Series

Note that this RMarkdown file is meant to be viewed in parallel with the Lecture slides.

#### Objectives: 

- Learn what a Time Series is, and be able to explain it.
- Be able to interpret Autocorrelation Function (ACF) and Partial Autocorrelation Function (PACF) graphs.
- Explain to a non-technical person the Seasonal Autoregressive Integrated Moving Average (SARIMA) model.
- Fit, analyze, and predict with an SARIMA model.

#### Outline:

1. What is a Time Series?
2. Stationarity
3. Autocovariance Function, Autocorrelation Function
4. Moving Average Model of Order q, or MA(q)
5. Autoregressive Model of Order p, or AR(p)
6. Differencing
7. Putting Together AR(p), MA(q), and Differencing. Or: Building the ARIMA(p,d,q) model.
8. Adding Seasonal Models SAR(P), MA(Q), and Seasonal Differencing.

## What is a Time Series?

We'll start with an example. Every day at work, you go and buy lunch at your favorite lunch spot. You have some number $X$, lets say $400$ which represents how much money you have in your account. We'll call the amount currently in your bank account $X_t$, where $t$ represents what day it is. Usually, you spend about \$15 (or, in statistical terms, your *mean* $\mu = -10$), but sometimes you spend +/- \$5 (or, in statistical terms, your *standard deviation* $\sigma^2 = 5$). We'll call this value $Z_t$, which represents what you would have spent on any given day. We can say the previous sentence more compactly, $Z_t \overset{iid}\sim N(\mu = -10, \sigma^2 = 5)$. Lets see what your bank account looks like after 100 days.

```{r lunch time series}
lunch_prices = rnorm(100, -10, 5)
starting_bank_account = 400
bank_account_balance = c(starting_bank_account)

for (i in 2:100) {
  # x_i = x_{i - 1} + z_i
  bank_account_balance[i] = bank_account_balance[i-1] + lunch_prices[i]
}

bank_account.ts = ts(bank_account_balance)

plot.ts(bank_account.ts, main = "Time Series of Lunch Spending", ylab = "Bank Account Balance", xlab = "Days")
```

There are plenty of other examples of Time Series as well.

- Daily temperature in an area
- A Business’s Quarterly Earnings
- Average rainfall per month
- The width of a tree over time
- Crime rate in a city in different months of the year
- Coronavirus confirmed cases over the last year.


So we can start to ask the question: what do these things have in common?

#### Time Series Definition:

 - *A (univariate) Time Series is a sequence of data over time, eg, $X_1, X_2, ... , X_T$, where T is the time period and $X_t$ is the value at a specific time stamp, $t \in T$. Typically, these time stamps are an equal distance apart in time.*


Before we jump into looking at more Time Series, lets motivate the discussion a little more:

We lay out 3 reasons why we favor the TS models for forecasting and analysis of this particular type of data:

  1. Works well with low-dimensional/sparse data.
  2. Easy to explain, as well as giving us insight into the relationship in the data
  3. Accounts for growing uncertainty as we move away from our ‘current time’

We'll show some more Time Series below:

```{r What are Time Series example 1}
plot.ts(jj, main = "Johnson and Johnson Quarterly Earnings Per Share", ylab = "Price, USD", xlab="Year")
```

```{r What are Time Series example 2}
windspeed.ts = ts(airquality[,'Wind'])

plot.ts(windspeed.ts, main = "Wind Speed in New York, Daily Measurement May to July 1973", ylab='Knots', xlab = "Days")
```

We can also generate our own time series, like we did at the beginning. We'll show two well known examples below:

We generate the White Noise time series via the equation $X_t = Z_t$ where $Z_t \overset{iid}\sim N(\mu, \sigma^2)$. In this case, $\mu = 0$ and $\sigma^2 = 1$.

```{r white noise}
wn = rnorm(1000)
wn.ts = ts(wn)

plot.ts(wn.ts, main="White Noise Time Series")
```

Below is the random walk. We actually used the equation shown here for our first example, but with different initial conditions! The random walk formula is $X_t = X_{t-1} + Z_t$, where $Z_t \overset{iid}\sim N(\mu, \sigma^2)$. Below, once again, $\mu = 0$ and $\sigma^2 = 1$.

```{r random walk}
z = rnorm(1000)
rw = c(0)

for (i in 2:1000) {
  rw[i] = rw[i-1] + z[i]
}

rw.ts = ts(rw)

plot.ts(rw.ts, main="Random Walk Time Series")
```

## Stochastic Processes



```{r Four White Noise example}
w = rnorm(1000)
w2 = rnorm(1000)
w3 = rnorm(1000)
w4 = rnorm(1000)

plot.ts(w, main = "Four White Noises", ylab = "Position")
lines(w2, col='red')
lines(w3, col='green')
lines(w4, col='blue')
```

```{r Four Random Walk example}
rw2 = c(0)
rw3 = c(0)
rw4 = c(0)
for (i in 2:1000) {
  rw[i] = rw[i-1] + w[i]
  rw2[i] = rw2[i-1] + w2[i]
  rw3[i] = rw3[i-1] + w3[i]
  rw4[i] = rw4[i-1] + w4[i]
}

plot.ts(rw, ylim=c(-15,60), main = "Four Random Walks", ylab = "Position")
lines(rw2, col='red')
lines(rw3, col='green')
lines(rw4, col='blue')
```

## Stationarity

```{r autocovariance function}
rw.ts = ts(rw)

acf(rw.ts, type = "covariance", main = "Autocovariance Plot of Random Walk")
acf(w, type="covariance", main = "Autocovariance Plot of White Noise")
```

```{r autocorrelation function}
acf(rw.ts, type = "correlation", main = "Autocorrelation Plot of Random Walk")
acf(w, type = "correlation", main = "Autocorrelation Plot of White Noise")
```
## Stationarity

We can run the Autocorrelation and Autocovariance functions to generate values for our data and visualize them.

```{r Autocovariance}
acf(wn.ts, type = "covariance", main = "Autocovariance Function Plot of White Noise TS")

acf(rw.ts, type = "covariance", main = "Autocovariance Function Plot of Random Walk TS")
```
Note that type = "correlation" is the default.

```{r Autocorrelation}
acf(wn.ts, type = "correlation", main = "Autocovariance Function Plot of White Noise TS")

acf(rw.ts, type = "correlation", main = "Autocovariance Function Plot of Random Walk TS")
```

## Moving Average Model, MA(q)

```{r MA(3) example}

z = rnorm(1000)
ma3.ts = c(0)

theta = c(0.7, 0.4, 0.1)

for (i in 4:1000) {
  ma3.ts[i-3] = z[i] + theta[1]*z[i-1] + theta[2]*z[i-2] + theta[3]*z[i-3]
}

plot.ts(ma3.ts, main="Moving Average 3 Model", ylab="Position")

acf(ma3.ts, type = "correlation", main = "MA(3) ACF plot")
```

Write a MA(2) model with $\theta_1 = 0.6$ and $\theta_2 = -0.3$. 

```{r MA(2) example, eval = FALSE}

z = rnorm(1000)

ma2.ts = c(0)

theta = c(0.6, -0.3)

for (i in 3:1000) {
  ma2.ts[i-2] = z[i] + theta[1] * z[i-1] + theta[2] * z[i-2]
}

plot.ts(ma2.ts, main="MA(2) Model", ylab="Position")
acf(ma2.ts, main = "MA(2) ACF Plot")
```

Write a MA(4) model with $\theta_1 = 0.9$, $\theta_2 = 0.5$, $\theta_3 = 0.3$, and $\theta_4 = 0.05$.

```{r MA(4) example, eval = FALSE}

# generate your random samples
z = rnorm(1000)
# initiate the ts vector
ma4.ts = c(0)
# assign thetas
theta = c(0.9, 0.5, 0.3, 0.2)
# loop the formula
for (i in 5:1000) {
  ma4.ts[i-4] = z[i] + theta[1] * z[i-1] + theta[2] * z[i-2] + theta[3] * z[i-3] + theta[4] * z[i-4]
}
# plot the TS and the ACF
plot.ts(ma4.ts)
acf(ma4.ts)
```


## Autoregressive Model of Order p, AR(p)

```{r AR(1) Random Walk}
z = rnorm(1000)
rw = vector(mode = "numeric", length = 1000)
rw[1] = 0
phi = c(1)

for (i in 2:1000) {
  rw[i] = phi[1]*rw.ts[i-1] + z[i]
}

rw.ts = ts(rw)

plot.ts(rw.ts, main="AR(1) Time Series")

acf(rw.ts, main = "AR(1) - Random Walk ACF")
acf(rw.ts, type = 'partial', main = "Partial Autocorrelation Function Plot (PACF), AR(1)")
```
We'll use a handy ARIMA simulator for the next examples. We will also use a slightly different AR

```{r AR(2) example 1}

phi = c(0.7,0.2)

ar2 = arima.sim(list(ar = c(phi[1], phi[2])), n = 1000)

plot.ts(ar2, main = "AR(2) Model with phi1 = 0.7, phi2 = 0.2", ylab = "Position")

acf(ar2, main = "ACF of AR(2) Model with phi1=0.7, phi2=0.2")
acf(ar2, type = "partial", main = "PACF of AR(2) Model with phi1=0.7, phi2=0.2")

acf2(ar2, main = "AR(2) Model with phi1 = 0.7, phi2 = 0.2")
```


Create an AR(2) model with $\phi_1 = 0.5, \phi_2 = -0.4$.

```{r AR(2) example 2, eval = FALSE}
phi = c(0.5, -0.4)

ar2 = arima.sim(list(ar = phi), n=1000)

plot.ts(ar2)

acf(ar2, type="partial", main = "AR(2) example, phi = 0.5, -0.4")

acf2(ar2)
```

Create an AR(3) model with $\phi_1 = 0.8, \phi_2 = -0.4, \phi_3 = 0.1$.

```{r AR(3) example 3, eval = FALSE}
# set phi
phi = c(0.8, -0.4, 0.2)
# use arima.sim to simulate the AR model
ar3 = arima.sim(list(ar=phi), n=1000)
# plot TS, ACF, PACF
plot.ts(ar3)

acf(ar3, type = "partial", main = "AR(3) Example, phi = 0.8, -0.4, 0.1")

acf2(ar3)
```

## Differencing

The command for differencing is simple:

```{r differencing random walk example}
rw_diff = diff(rw, 1)

plot.ts(rw_diff, main = 'Random Walk Differenced by 1', ylab = "Position")

acf(rw_diff, main = "ACF of Differenced Random Walk")

acf(rw_diff, type="partial", main ="PACF of Differenced Random Walk")
```

In the below example, we use differencing in order to attempt make the ACF plot more readable.

```{r differencing example ar2}
phi = c(0.7,0.2)

ar2 = arima.sim(list(ar = c(phi[1], phi[2])), n = 1000)

plot.ts(ar2, main = "AR(2) Model with phi1 = 0.7, phi2 = 0.2", ylab = "Position")

acf(ar2, main = "ACF of AR(2) Model with phi1=0.7, phi2=0.2")
acf(ar2, type = "partial", main = "PACF of AR(2) Model with phi1=0.7, phi2=0.2")
```

```{r differencing example ar2 part 2}
ar2.diff = diff(ar2, 1)

plot.ts(ar2.diff)

acf(ar2.diff)
acf(ar2.diff, type = "partial")
```
However, differencing too many times may introduce unwanted trends that can be invisible to the human eye:

```{r differencing example ar2 part 3}
ar2.diff.diff = diff(ar2.diff)

plot.ts(ar2.diff.diff)

acf(ar2.diff.diff)
acf(ar2.diff.diff, type="partial")
```


## Autoregressive Moving Average Model, ARIMA(p,d,q)

- We'll start by looking at BJ Sales indicator data.

```{r BJsales ARIMA 1}
library(datasets)

# investigate the data
?BJsales

# visualize, see trend
plot.ts(BJsales, main = "BJ Sales Data", ylab = "Sales")

# differencing, still see trend
plot.ts(diff(BJsales), main = "Differenced BJ Sales Data")

# difference a second time, still see trend
plot.ts(diff(diff(BJsales)), main = "Double Differenced BJ Sales Data")

# Ljung-box test for significant autocorrelation terms
bj.dd = diff(diff(BJsales))
Box.test(bj.dd, type="Ljung-Box", lag = log(length(bj.dd)))

acf(bj.dd, main = "ACF Double Differenced BJ Sales")

acf(bj.dd, type = 'partial', main = "PACF Double Differenced BJ Sales")
```


```{r BJsales ARIMA 2}
d=2
for(p in 1:4){
  for(q in 1:2){
    if(p+d+q<=6){
      model<-arima(x=BJsales, order = c((p-1),d,(q-1)))
      pval<-Box.test(model$residuals, lag=log(length(model$residuals)))
      sse<-sum(model$residuals^2)
      cat(p-1,d,q-1, 'AIC=', model$aic, ' SSE=',sse,' p-VALUE=', pval$p.value,'\n')
    }
  }
}

bj.arima = arima(BJsales, order = c(0,2,1))
 
bj.predict = forecast(bj.arima, h=12, level=80)

autoplot(bj.predict, main = "ARIMA(0,2,1) Prediction on BJ Sales", ylab="Sales")
```


- We'll be looking at the WWWusage data set from the datasets library. This measures the number of active users on a server over 100 minutes.

```{r WWWusage ARIMA 1}
WWWusage

plot.ts(WWWusage)

plot.ts(diff(WWWusage))

WWWusage.diff = diff(WWWusage)

plot.ts(diff(diff(WWWusage)))

WWWusage.dd = diff(WWWusage.diff)

Box.test(WWWusage.dd, lag = log(length(WWWusage.dd)))

acf2(WWWusage.dd)

acf2(WWWusage.diff)

```


```{r WWWusage ARIMA 2}

d = 2
# p order 2
# q order 3
# d order 1
for (p in 1:3) {
  for (q in 1:4) {
    if (p + q + d <= 11) {
      model = arima(x=WWWusage, order = c((p-1), d,(q-1)))
      pval = Box.test(model$residuals, lag = log(length(model$residuals)))
      sse = sum(model$residuals^2)
      cat(p-1,d,q-1, "AIC=", model$aic, " SSE =", sse, " p-VALUE=", pval$p.value, "\n")
    }
  }
}
# 2 2 0
WWW.arima = arima(x=WWWusage, order = c(2,2,0))

WWW.predict = forecast(WWW.arima, h=10, level=80)

autoplot(WWW.predict, main = "ARIMA(3,1,0) Prediction on WWW Usage Minute-to-Minute")
```


- Next we have Air Passenger Data from ~1948 to ~1970.

```{r AirPassenger ARIMA 1}
?AirPassengers

plot.ts(AirPassengers)

AP.log = log(AirPassengers)

plot.ts(AP.log)

plot.ts(diff(AP.log))

Box.test(diff(AP.log), lag = log(length(diff(AP.log))))

AP.log.diff = diff(AP.log)

acf2(AP.log.diff)

```

```{r AirPassenger ARIMA 2}
# for now...
# p = 1
# q = 1
# d = 1

d = 1
for (p in 1:2) {
  for (q in 1:2) {
    if (p + q + d <= 5) {
      model = arima(x=AP.log, order = c((p-1), d,(q-1)))
      pval = Box.test(model$residuals, lag = log(length(model$residuals)))
      sse = sum(model$residuals^2)
      cat(p-1,d,q-1, "AIC=", model$aic, " SSE =", sse, " p-VALUE=", pval$p.value, "\n")
    }
  }
}

AP.arima = arima(x=AP.log, order = c(1,1,1))

AP.predict = forecast(AP.arima, h=12, level=80)

autoplot(AP.predict)
```

## Seasonal Autoregressive Integrated Moving Average Model, SARIMA(p,d,q,P,D,Q)S

- Our above prediction was pretty bad, can we do better?

```{r seasonal differencing}
plot.ts(diff(AP.log[1:48]))

# in order to difference seasonally, we use the second argument of the diff function.
AP.log.seasonaldiff = diff(diff(AP.log), 12)

plot.ts(AP.log.seasonaldiff[1:48])
```
```{r AirPassengers SARIMA continued}
plot.ts(AP.log.diff)

AP.log.seasonal.diff = diff(AP.log.diff, 12)

plot.ts(AP.log.seasonal.diff)

Box.test(AP.log.seasonal.diff, lag = log(length(AP.log.seasonal.diff)))

acf2(AP.log.seasonal.diff)

acf(AP.log.seasonal.diff)
```


```{r AirPassengers SARIMA 2}
# looks like:
# p = 1
# q = 1
# d = 1
# p_seasonal = 1
# DD = 1
# Q_seasonal = 1
# S = 12

d = 1
DD = 1
per = 12
for(p in 1:2){
  for(q in 1:2){
    for(p_seasonal in 1:2){
      for(q_seasonal in 1:2){
        if(p+d+q+p_seasonal+DD+q_seasonal<=11){
          model<-arima(x=AP.log, order = c((p-1),d,(q-1)), seasonal = list(order=c((p_seasonal-1),DD,(q_seasonal-1)), period=per))
          pval<-Box.test(model$residuals, lag=log(length(model$residuals)))
          sse<-sum(model$residuals^2)
          cat(p-1,d,q-1,p_seasonal-1,DD,q_seasonal-1,per, 'AIC=', model$aic, ' SSE=',sse,' p-VALUE=', pval$p.value,'\n')
        }
      }
    }
  }
}

AP.sarima = arima(x=AP.log, order = c(0,1,1), seasonal = list(order = c(0,1,1), period = per))

AP.predict = forecast(AP.sarima, h=24, level = 80)

autoplot(AP.predict)
```


- Now we'll return to an early example, the Johnson and Johnson Quarterly Stock Return (per stock, in USD) from YYYY to YYYY.

```{r jj sarima example}
plot.ts(jj, main = "Johnson and Johnson Quarterly Stock Data", ylab="Price (USD)")

plot.ts(jj[1:16], main = "Johnson and Johnson Quarterly Stock Data (Zoomed)", ylab="Price (USD)")

plot.ts(log(jj), main = "Logged J&J Quarterly Stock Data", ylab="Log-Price")

plot.ts(diff(log(jj)), main = "Log-Return J&J Quarterly Stock Data", ylab="Log-Return")

plot.ts(diff(diff(log(jj)), 4), main = "Log-Return J&J Quarterly Stock Data (Non-Seasonal)", ylab = "Log-Return")

jj.diff = diff(log(jj))

jj.stat = diff(diff(log(jj)),4)

acf2(jj.diff, main = "ACF, PACF of Log-Return J&J Data")

acf2(jj.stat, main = "ACF, PACF Non-Seasonal and Seasonally Detrended J&J Data")

# Have to test before we 
Box.test(diff(diff(log(jj)),4), lag = log(length(diff(diff(log(jj)),4))))
d=1
DD=1

per=4

for(p in 1:2){
  for(q in 1:2){
    for(i in 1:2){
      for(j in 1:2){
        if(p+d+q+i+DD+j<=11){
          model<-arima(x=log(jj), order = c((p-1),d,(q-1)), seasonal = list(order=c((i-1),DD,(j-1)), period=per))
          pval<-Box.test(model$residuals, lag=log(length(model$residuals)))
          sse<-sum(model$residuals^2)
          cat(p-1,d,q-1,i-1,DD,j-1,per, 'AIC=', model$aic, ' SSE=',sse,' p-VALUE=', pval$p.value,'\n')
        }
      }
    }
  }
}

# 0 1 1 1 1 0 4

library(astsa)

jj.arima = arima(x=log(jj), order = c(0,d,1), seasonal = list(order=c(1,DD,0), period=per))

jj.forecast = forecast(jj.arima, h=8, level=80)

# more detailed output
sarima(log(jj), 0, d, 1, 1, DD, 0, per)

autoplot(jj.forecast, main = "2 Year Forecast on Logged Johnson and Johnson Data")
```

```{r nottem SARIMA 1}
?nottem

plot.ts(nottem)

nottem.log = log(nottem)

plot.ts(nottem.log[1:48])

plot.ts(diff(nottem.log, 12))

plot.ts(diff(diff(log(nottem)), 12))

Box.test(diff(diff(log(nottem),12)), lag = log(length(diff(diff(nottem.log,12)))))

nottem.dd.log = diff(diff(nottem.log, 12))

acf2(nottem.dd.log)

``` 
Looks like:
- p = 7
- q = 1
- d = 1
- P = 2
- D = 1
- Q = 2
- S = 12

```{r nottem SARIMA 2}

d=1
DD=1

per=12

for(p in 1:4){
  for(q in 1:2){
    for(i in 1:3){
      for(j in 1:2){
        if(p+d+q+i+DD+j<=13){
          model<-arima(x=nottem.log, order = c((p-1),d,(q-1)), seasonal = list(order=c((i-1),DD,(j-1)), period=per))
          pval<-Box.test(model$residuals, lag=log(length(model$residuals)))
          sse<-sum(model$residuals^2)
          cat(p-1,d,q-1,i-1,DD,j-1,per, 'AIC=', model$aic, ' SSE=',sse,' p-VALUE=', pval$p.value,'\n')
        }
      }
    }
  }
}
```

We end up getting SARIMA(1,1,1,2,1,1)12 with an AIC of $-682.0964$, so we'll generate the model below, and predict:

```{r nottem SARIMA 3}
nottem.sarima = arima(x=nottem.log, order = c(1,1,1), seasonal = list(order = c(2,1,1), per = 12))

nottem.predict = forecast(nottem.sarima, h=12, level = 80)

autoplot(nottem.predict)
```